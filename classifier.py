# -*- coding: utf-8 -*-
"""HW2question3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zG04yw4aR_M5-2J84LJ4VrWtv5C3E7xz
"""

from google.colab import drive
drive.mount('/content/gdrive')

import os
import numpy as np
import pandas as pd

root = '/content/gdrive/My Drive/dataset'
train_features_csv_path = os.path.join(root, 'features-train.csv')
train_labels_csv_path = os.path.join(root, 'labels-train.csv')
test_features_csv_path = os.path.join(root, 'features-test.csv')
test_labels_csv_path = os.path.join(root, 'labels-test.csv')

train_features = pd.read_csv(train_features_csv_path)
train_labels = pd.read_csv(train_labels_csv_path)
test_features = pd.read_csv(test_features_csv_path)
test_labels = pd.read_csv(test_labels_csv_path)

train_features = (train_features - np.mean(train_features, axis=0)) / np.std(train_features, axis=0)
test_features = (test_features - np.mean(test_features, axis=0)) / np.std(test_features, axis=0)

train_features = train_features.to_numpy()
train_labels = train_labels.to_numpy()
test_features = test_features.to_numpy()
test_labels = test_labels.to_numpy()

def gradientAscent(train_features, train_labels, learning_rate, iterations):
  # get number of samples and features
  samples, features = train_features.shape
  weights = np.zeros(features)
  bias = 0

  # gradient ascent
  for i in range(iterations):
    model = np.dot(train_features, weights) + bias
    new_labels = 1 / (1 + np.exp(model))

    derivative_w = (1 / samples) * np.dot(train_features.T, new_labels - train_labels)
    derivative_b = (1 / samples) * np.sum(new_labels - train_labels)

    weights += learning_rate  * derivative_w
    bias += learning_rate  * derivative_b

    return weights, bias

# gradient batch ascent
def gradientBatchAscent(train_features, train_labels, learning_rate, iterations, batch_size):
  # get number of samples and features
  samples, features = train_features.shape
  weights = np.zeros(features)
  bias = 0
  start = 0

  # gradient batch ascent
  for i in range(iterations):
    for j in range(int(train_features.shape[0] / batch_size)):
      train_features = train_features[start : start + batch_size]
      train_labels = train_labels[start : start + batch_size]
      model = np.dot(train_features, weights) + bias
      new_labels = 1 / (1 + np.exp(model))

      derivative_w = (1 / samples) * np.dot(train_features.T, new_labels - train_labels)
      derivative_b = (1 / samples) * np.sum(new_labels - train_labels)

      weights -= learning_rate  * derivative_w
      bias -= learning_rate  * derivative_b

      start = (start + batch_size) % train_features.shape[0]

  return weights, bias

def predict(train_features, weights, bias):
  model = np.dot(train_features, weights) + bias
  new_labels = 1 / (1 + np.exp(-model))

  predictions = ((new_labels >= 0.5).astype(int)).flatten()
  return predictions

def print_accuracy(predictions, test_labels):
  TP = 0
  TN = 0
  FP = 0
  FN = 0
  TP = sum(predictions[np.where(predictions == test_labels)] == 1)
  TN = sum(predictions[np.where(predictions == test_labels)] == 0)
  FP = sum(predictions[np.where(predictions != test_labels)] == 1)
  FN = sum(predictions[np.where(predictions != test_labels)] == 0)
  accuracy = ((TP + TN) / (TP + TN + FP + FN)) * 100
  print('Accuracy ', accuracy)
  print("Confusion Matrix")
  print("TP: ", TP, "FP: ", FP)
  print("FN: ", FN, "TN: ", TN)
  precision = TP / (TP + FP)
  print("Precision ", precision)
  recall = TP / (TP + FN)
  print("Recall ", recall)
  NPV = TN / (FN + TN)
  print("NPV ", NPV)
  FPR = FP / (TP + FP)
  print("FPR ", FPR)
  FDR = FP / (FN + TN)
  print("FDR ", FDR)
  F1 = 2 * precision * recall / (precision + recall)
  print("F1 ", F1)
  F2 = 5 * precision * recall / ((4 * precision) + recall)
  print("F2 ", F2)

w, b = gradientAscent(train_features, train_labels.ravel(), 0.001, 1000)
predictions = predict(test_features, w, b)
print_accuracy(predictions, test_labels.ravel())

# mini-batch gradient ascent with batchsize 100
w, b = gradientBatchAscent(train_features, train_labels.ravel(), 0.001, 1000, 100)
predictions = predict(test_features, w, b)
print_accuracy(predictions, test_labels.ravel())